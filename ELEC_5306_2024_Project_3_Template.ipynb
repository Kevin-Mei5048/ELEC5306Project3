{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUmYpQUBHN1W"
      },
      "source": [
        "Project 3 Part B: A Deep Learning Toy Model for Video Compression\n",
        "===\n",
        "\n",
        "In this project, we will use simple deep learning networks for basic video compression.\n",
        "\n",
        "For this project, we are going to use a template that's mostly from project 2. The reason is that a video is essentially a sequence of images, in which case a network for image compression is much likely to be re-usable for video compression.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Objectives, Requirements and Others\n",
        "\n",
        "### 1.1 Objectives\n",
        "\n",
        "* Customize your model and training process for better performance -- High PSNR on test videos.\n",
        "* Make sure the model complexity not too high and compression ratio not too low.\n",
        "\n",
        "### 1.2 Requirements\n",
        "\n",
        "* Project 3 is an **Group Project**. The report should include the team member contributions\n",
        "* **Training data** can only be the data under `Image` and `Video/train` folders that provided.\n",
        "* Model limit: there's a **benchmark test** at the end of this notebook. It includes model complexity test and compression ratio test. Please **test your model with this and make sure it satisfies the requirement both in complexity and compression ratio**. \n",
        "* **NO pre-trained model**. You are supposed to train the network from scratch. \n",
        "\n",
        "### 1.3 Submission\n",
        "\n",
        "Each student is supposed to upload a **{YOUR_FIRST_NAME}_{YOUR_UNIKEY}.zip** (e.g. Sydney_haha2333.zip) file with three files:\n",
        "\n",
        "* **a jupyternotebook** with the complete training and testing details of your final model. In other words, it is expected to reproduce the complete experiment process and reproduce the result on ```test``` datatest of your final model.\n",
        "* **a .pth file** contains the trained weight of your model.\n",
        "* **a PDF report** contains at least the following section:\n",
        "  * Introduction\n",
        "  * Method\n",
        "  * Experiment & Analysis\n",
        "  * Conclusion\n",
        "  * (Further details are strongly encouraged, especially experiment analysis and the novelty of the method)\n",
        "  * (Tables and Figures are encouraged)\n",
        "\n",
        "### 1.4 Marking Scheme\n",
        "* Code -- 25%\n",
        "  * the submitted code could run without error (except the path to the dataset) -- 15%\n",
        "  * the complete experimental output and log of each code cell -- 5%\n",
        "  * proper comment that makes the code easy to read -- 5%\n",
        "\n",
        "* PDF report -- 45%\n",
        "  * well-written Introduction -- 5%\n",
        "  * detailed discription of the method (e.g. includes an overview of the network structure of your method) -- 15%\n",
        "  * detailed in-depth **quantitative** and **qualitative** analysis (e.g. tables and figures of results, how you go from baseline to the final model step-by-step and how each step affects the performance) -- 15%\n",
        "  * good performance of the model -- 10%\n",
        "\n",
        "* Presentation -- 30% \n",
        "  * easy-to-follow presentation -- 15%\n",
        "  * detailed in-depth **quantitative** and **qualitative** analysis -- 15%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzs2kueOgxN3"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Preparation for the experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkzEhhMfA-_b"
      },
      "source": [
        "### 2.1 Prepare the data\n",
        "\n",
        "Please refer to the data provided in Assignment 2\n",
        "\n",
        "**NOTE:** \n",
        "\n",
        "1. The image data part is exactly the same training data used for project 2\n",
        "2. **ONLY data in image and video/train can be used for training**, no other sources allowed.\n",
        "\n",
        "Step-by-step\n",
        "\n",
        "1. Put those folder (and the data inside of course) in your GoogleDrive.\n",
        "2. Run the command below to mount the GoogleDrive onto Colab.\n",
        "3. Check the path to the data (e.g. `'/content/drive/MyDrive/MYDATA'`), and we will use this later.\n",
        "\n",
        "Hint: you can check the file with either bash command or use the leftmost sidebar in Colab, there's a folder shaped icon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Rv7d8v8O7_"
      },
      "source": [
        "You can use drive.mount() to mount your Google Drive to the Colab file. For other usages, please refer to previous Ed posts and announcements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bgKCJIe9jZh",
        "outputId": "b8210a2f-0218-4e43-caf8-5bef639d96aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GC-BrL3kmoN"
      },
      "source": [
        "### 2.2 Preliminary: Conversion between images and videos\n",
        "\n",
        "A video is essentially a sequence of consecutive images. We can convert a video to images and vice versa. Here is an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYn0j9DqlZxM",
        "outputId": "31a5d832-c920-4c11-9fea-1e2c9271e3cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "success\n"
          ]
        }
      ],
      "source": [
        "# convert image sequence to a video\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        " \n",
        "path_to_sequence = '/Your-path/ELEC5306/Video/train/Bird2/img'\n",
        "img_array = []\n",
        "\n",
        "# remember to sort the images before conversion, \n",
        "# otherwise they might be in wrong temporal order\n",
        "for filename in sorted(glob.glob(os.path.join(path_to_sequence, '*.jpg'))):\n",
        "    img = cv2.imread(filename)\n",
        "    height, width, layers = img.shape\n",
        "    size = (width,height)\n",
        "    img_array.append(img)\n",
        " \n",
        "out = cv2.VideoWriter('/Your-path/ELEC5306/Video/train/Bird2/video.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
        " \n",
        "for i in range(len(img_array)):\n",
        "    out.write(img_array[i])\n",
        "out.release()\n",
        "print('success')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or76XY_orfou",
        "outputId": "a7bd9bee-699c-4d55-879a-2200847e45b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "end of sequence\n"
          ]
        }
      ],
      "source": [
        "# convert video to image sequence\n",
        "\n",
        "def getFrame(vid_path, out_dir):\n",
        "    vidcap = cv2.VideoCapture(vid_path)\n",
        "    hasFrames = True\n",
        "    count = 0\n",
        "    while hasFrames:\n",
        "      hasFrames,image = vidcap.read()\n",
        "      if hasFrames:\n",
        "        cv2.imwrite(os.path.join(out_dir, \"image_\"+str(count)+\".jpg\"), image)     # save frame as JPG file\n",
        "        count += 1\n",
        "      else:\n",
        "        print('end of sequence')\n",
        "        break\n",
        "\n",
        "getFrame(\n",
        "    '/Your-path/ELEC5306/Video/train/Bird2/video.mp4',\n",
        "    '/Your-path/ELEC5306/Video/train/Bird2/img_from_video')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAsWk6l-xuim"
      },
      "source": [
        "### 2.3 Get GPU ready\n",
        "\n",
        "Change Runtime Type in the Colab menu to 'GPU'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byw0cexcCqiM"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Code Template\n",
        "\n",
        "Here is a template for training and testing a baseline model. The model is the same one we use for the Project 2. The training process are the same, while the testing are performed on video sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDgQqbjXeTfn"
      },
      "source": [
        "Let's firt import packages for this project. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4qJclN79EecH"
      },
      "outputs": [],
      "source": [
        "# ################################################\n",
        "# import packages\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owR0dmNvfQmW"
      },
      "source": [
        "Then we define a Dataset class that could iterate over images under a directory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kUZuUOkVE6Qw"
      },
      "outputs": [],
      "source": [
        "# ################################################\n",
        "# ImageFolder Dataset\n",
        "\n",
        "class ImageFolder(Dataset):\n",
        "    \"\"\"Load an image folder database. Training and testing image samples\n",
        "    are respectively stored in separate directories:\n",
        "\n",
        "    .. code-block::\n",
        "\n",
        "        - rootdir/\n",
        "            - train/\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "            - valid/\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "\n",
        "    Args:\n",
        "        root (string): root directory of the dataset\n",
        "        transform (callable, optional): a function or transform that takes in a\n",
        "            PIL image and returns a transformed version\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None):\n",
        "        splitdir = Path(root)\n",
        "\n",
        "        if not splitdir.is_dir():\n",
        "            raise RuntimeError(f'Invalid directory \"{root}\"')\n",
        "\n",
        "        self.samples = [f for f in splitdir.iterdir() if f.is_file()]\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            img: `PIL.Image.Image` or transformed `PIL.Image.Image`.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.samples[index]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            return self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lBKEXKmjChPU"
      },
      "outputs": [],
      "source": [
        "# ################################################\n",
        "# SequenceFolder Dataset\n",
        "\n",
        "class SequenceFolder(Dataset):\n",
        "    \"\"\"Load an image folder database. Training and testing image samples\n",
        "    are respectively stored in separate directories:\n",
        "\n",
        "    .. code-block::\n",
        "\n",
        "        - rootdir/\n",
        "            - train/\n",
        "              - video 1\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "              - video 2\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "            - test/\n",
        "              - video 1\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "              - video 2\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "\n",
        "    Args:\n",
        "        root (string): root directory of the dataset\n",
        "        transform (callable, optional): a function or transform that takes in a\n",
        "            PIL image and returns a transformed version\n",
        "        split (string): split mode ('train' or 'val')\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None, split=\"train\"):\n",
        "        self.mode = split\n",
        "        splitdir = Path(root) / split\n",
        "\n",
        "        if not splitdir.is_dir():\n",
        "            raise RuntimeError(f'Invalid directory \"{root}\"')\n",
        "\n",
        "        self.samples = self.get_all_images(splitdir)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def get_all_images(self, direc):\n",
        "        self.images = []\n",
        "        self.image_sequence = []\n",
        "        self.sequences = [f for f in direc.iterdir() if f.is_dir()]\n",
        "        for sd in self.sequences:\n",
        "            images = []\n",
        "            for f in (sd / 'img').iterdir():\n",
        "                if f.is_file():\n",
        "                  images.append(f)\n",
        "            self.image_sequence.append(images)\n",
        "            self.images = self.images + list(sorted(images))\n",
        "\n",
        "        return self.images\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            img: `PIL.Image.Image` or transformed `PIL.Image.Image`.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.samples[index]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            return self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HtrHCpgFCeK",
        "outputId": "be0f9518-407a-4201-b658-7840c09988e8"
      },
      "outputs": [],
      "source": [
        "d = SequenceFolder('/Your-path/ELEC5306/Video')\n",
        "d.image_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq1tuazucdCr"
      },
      "source": [
        "Then let's define a baseline neural network.\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "As in the requirement, please use the ```benchmark``` function at the end of this template to check your model's complexity and compression ratio, and see if both meet the requirement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jIDZbA0JE8cQ"
      },
      "outputs": [],
      "source": [
        "# ###################################################\n",
        "# a baseline model\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size=5, stride=2):\n",
        "    return nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        stride=stride,\n",
        "        padding=kernel_size // 2,\n",
        "    )\n",
        "\n",
        "\n",
        "def deconv(in_channels, out_channels, kernel_size=5, stride=2):\n",
        "    return nn.ConvTranspose2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        stride=stride,\n",
        "        output_padding=stride - 1,\n",
        "        padding=kernel_size // 2,\n",
        "    )\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, N, M, init_weights=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.g_a = nn.Sequential(\n",
        "            conv(3, N),\n",
        "            conv(N, N),\n",
        "            conv(N, N),\n",
        "            conv(N, M),\n",
        "        )\n",
        "\n",
        "        self.g_s = nn.Sequential(\n",
        "            deconv(M, N),\n",
        "            deconv(N, N),\n",
        "            deconv(N, N),\n",
        "            deconv(N, 3),\n",
        "        )\n",
        "\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.g_a(x)\n",
        "        x_hat = self.g_s(y)\n",
        "\n",
        "        return {\n",
        "            \"x_hat\": x_hat,\n",
        "        }\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def compress(self, x):\n",
        "        y = self.g_a(x)\n",
        "        return y\n",
        "\n",
        "    def decompress(self, y_hat):\n",
        "        x_hat = self.g_s(y_hat).clamp_(0, 1)\n",
        "        return {\"x_hat\": x_hat}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd9e3NRtlgi2"
      },
      "source": [
        "Here are some other ingredients for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pRlDgjicFtdN"
      },
      "outputs": [],
      "source": [
        "class Loss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        out = {}\n",
        "        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n",
        "        out[\"loss\"] = out[\"mse_loss\"] * 255\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MF1C9J_NYYoe"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Compute running average.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rwmPi5mebRu8"
      },
      "outputs": [],
      "source": [
        "def configure_optimizers(net, learning_rate):\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        net.parameters(),\n",
        "        lr=learning_rate,\n",
        "    )\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-rJt2-91bv4c"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        print('Saving best model...')\n",
        "        shutil.copyfile(filename, \"checkpoint_best_loss.pth.tar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rKc3sUFXPK5d"
      },
      "outputs": [],
      "source": [
        "def PSNR(img1, img2):\n",
        "    # img1 and img2 within range [0, 1]\n",
        "    # img1 shape: (B, C, H, W)\n",
        "    # img2 shape: (B, C, H, W)\n",
        "    \n",
        "    img1, img2 = img1.detach(), img2.detach()\n",
        "    img1 = img1 * 255\n",
        "    img2 = img2 * 255\n",
        "    batch_size = img1.shape[0]\n",
        "    img1 = img1.reshape(batch_size, -1)\n",
        "    img2 = img2.reshape(batch_size, -1)\n",
        "    mse = torch.mean((img1 - img2) ** 2)\n",
        "    return torch.mean(20 * torch.log10(255.0 / torch.sqrt(mse)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCZLieuUjIn5"
      },
      "source": [
        "After we almost got everything we need, we define a `train_one_epoch` function for training the model for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M-J38tMfbiFR"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model, criterion, train_dataloader, optimizer, epoch, clip_max_norm\n",
        "):\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for i, d in enumerate(train_dataloader):\n",
        "        d = d.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out_net = model(d)\n",
        "\n",
        "        out_criterion = criterion(out_net, d)\n",
        "        out_criterion[\"loss\"].backward()\n",
        "        if clip_max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(\n",
        "                f\"Train epoch {epoch}: [\"\n",
        "                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n",
        "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
        "                f'\\tLoss: {out_criterion[\"loss\"].item():.3f} |'\n",
        "                f'\\tMSE loss: {out_criterion[\"mse_loss\"].item():.3f}'\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J61e5WBajaz7"
      },
      "source": [
        "Likewise we further define a `test_epoch` function. \n",
        "\n",
        "Unlike project 2, this time the metric is measured on videos instead of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kdTl131jbqnX"
      },
      "outputs": [],
      "source": [
        "def test_epoch(epoch, test_dataset, transform, model, criterion):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    mse_loss = AverageMeter()\n",
        "    psnr = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, video in enumerate(test_dataset.image_sequence):\n",
        "            psnr_video = AverageMeter()\n",
        "            for j, image in enumerate(video):\n",
        "                image = Image.open(image).convert(\"RGB\")\n",
        "                image = transform(image)\n",
        "                d = image.to(device).unsqueeze(0)\n",
        "                out_net = model(d)\n",
        "                d_out = out_net['x_hat']\n",
        "                out_criterion = criterion(out_net, d)\n",
        "                psnr_video.update(PSNR(d, d_out))\n",
        "\n",
        "            mse_loss.update(out_criterion[\"mse_loss\"])\n",
        "            psnr.update(psnr_video.avg)\n",
        "\n",
        "    print(\n",
        "        f\"Test epoch {epoch}: Average losses:\"\n",
        "        f\"\\tMSE loss: {mse_loss.avg:.3f}\"\n",
        "        f'\\tSequence-wise PSNR: {psnr.avg: .3f}\\n'\n",
        "    )\n",
        "\n",
        "    return mse_loss.avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9KqN_qlkDB0"
      },
      "source": [
        "Finally we define a main function, and complete the whole training/testing process.\n",
        "\n",
        "At the start of this main function, we define some arguments that's useful for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x63ecQUskvZ7",
        "outputId": "c3fb5b8c-bafc-4074-d2bc-caa599d7e67a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train  valid  Video\n"
          ]
        }
      ],
      "source": [
        "!ls /Your-path/ELEC5306"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk512yTjv7Lh"
      },
      "source": [
        "Define some arguments useful for training/validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-h-p6LA_bw99"
      },
      "outputs": [],
      "source": [
        "seed = 123                                        # for reproducibility\n",
        "cuda = True                                       # use GPU\n",
        "save = True                                       # save trained model\n",
        "image_dataset = '/Your-path/ELEC5306/train'  # path to the root of the image dataset\n",
        "sequence_dataset = '/Your-path/ELEC5306/Video'  # path to the root of the video dataset\n",
        "checkpoint = ''                                   # load pretrained model\n",
        "epochs = 10                                       # total training epochs\n",
        "clip_max_norm = 1.0                               # avoid gradient explosion\n",
        "patch_size = (256, 256)                           # input size for the training network\n",
        "learning_rate = 1e-4  \n",
        "batch_size = 16\n",
        "test_batch_size = 16\n",
        "num_workers = batch_size                          # multi-process for loading training data \n",
        "N = 128\n",
        "M = 192"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHEARHw8v56D",
        "outputId": "8c2388ab-faae-4f76-8ffe-7d2637a78704"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate: 0.0001\n",
            "Train epoch 0: [0/250 (0%)]\tLoss: 4425.690 |\tMSE loss: 17.356\n",
            "Train epoch 0: [160/250 (62%)]\tLoss: 149.755 |\tMSE loss: 0.587\n",
            "Test epoch 0: Average losses:\tMSE loss: 0.466\tSequence-wise PSNR:  3.957\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 1: [0/250 (0%)]\tLoss: 55.701 |\tMSE loss: 0.218\n",
            "Train epoch 1: [160/250 (62%)]\tLoss: 57.169 |\tMSE loss: 0.224\n",
            "Test epoch 1: Average losses:\tMSE loss: 0.265\tSequence-wise PSNR:  6.476\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 2: [0/250 (0%)]\tLoss: 50.268 |\tMSE loss: 0.197\n",
            "Train epoch 2: [160/250 (62%)]\tLoss: 40.541 |\tMSE loss: 0.159\n",
            "Test epoch 2: Average losses:\tMSE loss: 0.172\tSequence-wise PSNR:  8.516\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 3: [0/250 (0%)]\tLoss: 32.223 |\tMSE loss: 0.126\n",
            "Train epoch 3: [160/250 (62%)]\tLoss: 29.527 |\tMSE loss: 0.116\n",
            "Test epoch 3: Average losses:\tMSE loss: 0.114\tSequence-wise PSNR:  10.256\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 4: [0/250 (0%)]\tLoss: 14.204 |\tMSE loss: 0.056\n",
            "Train epoch 4: [160/250 (62%)]\tLoss: 19.555 |\tMSE loss: 0.077\n",
            "Test epoch 4: Average losses:\tMSE loss: 0.078\tSequence-wise PSNR:  11.935\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 5: [0/250 (0%)]\tLoss: 6.375 |\tMSE loss: 0.025\n",
            "Train epoch 5: [160/250 (62%)]\tLoss: 11.677 |\tMSE loss: 0.046\n",
            "Test epoch 5: Average losses:\tMSE loss: 0.060\tSequence-wise PSNR:  12.951\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 6: [0/250 (0%)]\tLoss: 16.279 |\tMSE loss: 0.064\n",
            "Train epoch 6: [160/250 (62%)]\tLoss: 6.317 |\tMSE loss: 0.025\n",
            "Test epoch 6: Average losses:\tMSE loss: 0.048\tSequence-wise PSNR:  14.001\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 7: [0/250 (0%)]\tLoss: 9.591 |\tMSE loss: 0.038\n",
            "Train epoch 7: [160/250 (62%)]\tLoss: 7.948 |\tMSE loss: 0.031\n",
            "Test epoch 7: Average losses:\tMSE loss: 0.041\tSequence-wise PSNR:  14.672\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 8: [0/250 (0%)]\tLoss: 6.070 |\tMSE loss: 0.024\n",
            "Train epoch 8: [160/250 (62%)]\tLoss: 8.863 |\tMSE loss: 0.035\n",
            "Test epoch 8: Average losses:\tMSE loss: 0.033\tSequence-wise PSNR:  15.724\n",
            "\n",
            "Saving best model...\n",
            "Learning rate: 0.0001\n",
            "Train epoch 9: [0/250 (0%)]\tLoss: 12.445 |\tMSE loss: 0.049\n",
            "Train epoch 9: [160/250 (62%)]\tLoss: 5.071 |\tMSE loss: 0.020\n",
            "Test epoch 9: Average losses:\tMSE loss: 0.028\tSequence-wise PSNR:  16.474\n",
            "\n",
            "Saving best model...\n",
            "the overall training time (exclude testing) is 3.8381633162498474 min\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "train_transforms = transforms.Compose(\n",
        "    [transforms.RandomCrop(patch_size), transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [transforms.CenterCrop(patch_size), transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "train_dataset = ImageFolder(image_dataset, transform=train_transforms)\n",
        "test_dataset = SequenceFolder(sequence_dataset, split=\"test\", transform=None)\n",
        "\n",
        "device = \"cuda\" if cuda and torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=True,\n",
        "    pin_memory=(device == \"cuda\"),\n",
        ")\n",
        "\n",
        "net = Network(N, M)\n",
        "net = net.to(device)\n",
        "\n",
        "optimizer = configure_optimizers(net, learning_rate)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
        "criterion = Loss()\n",
        "\n",
        "last_epoch = 0\n",
        "if checkpoint:  # load from previous checkpoint\n",
        "    print(\"Loading\", checkpoint)\n",
        "    checkpoint = torch.load(checkpoint, map_location=device)\n",
        "    last_epoch = checkpoint[\"epoch\"] + 1\n",
        "    net.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "train_time = AverageMeter()\n",
        "for epoch in range(last_epoch, epochs):\n",
        "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "    epoch_train_start = time.time()\n",
        "    train_one_epoch(\n",
        "        net,\n",
        "        criterion,\n",
        "        train_dataloader,\n",
        "        optimizer,\n",
        "        epoch,\n",
        "        clip_max_norm,\n",
        "    )\n",
        "    epoch_train_end = time.time()\n",
        "    train_time.update(epoch_train_end - epoch_train_start)\n",
        "    loss = test_epoch(epoch, test_dataset, test_transforms, net, criterion)\n",
        "    lr_scheduler.step(loss)\n",
        "\n",
        "    is_best = loss < best_loss\n",
        "    best_loss = min(loss, best_loss)\n",
        "\n",
        "    if save:\n",
        "      save_checkpoint(\n",
        "          {\n",
        "              \"epoch\": epoch,\n",
        "              \"state_dict\": net.state_dict(),\n",
        "              \"loss\": loss,\n",
        "              \"optimizer\": optimizer.state_dict(),\n",
        "              \"lr_scheduler\": lr_scheduler.state_dict(),\n",
        "          },\n",
        "          is_best,\n",
        "      )\n",
        "print('the overall training time (exclude testing) is {} min'.format(train_time.sum / 60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxSwpCHlmNWV"
      },
      "source": [
        "### Benchmark Test (VERY IMPORTANT)\n",
        "\n",
        "1. Please use 'torch.save(model.state_dict(), PATH)' to save your trained model\n",
        "\n",
        "2. Please use the below code block, to instantiate your model as object 'test_model' for marking. We will use 'load_state_dict()' to load your model. Please make sure your saved model can pass the loading test. You can use the below code block to test.\n",
        "\n",
        "3. Please include your trained model weight in the submission.\n",
        "\n",
        "4. **If we cannot load your uploaded weight, you will get zero mark for this assignment.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "5ry2cD21p_ms",
        "outputId": "4ba28719-9da6-4d5c-c404-50f9cc426b83"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7485d74e3d09>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoint_best_loss.pth.tar'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYourModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'YourModel' is not defined"
          ]
        }
      ],
      "source": [
        "### EDIT REQUIRED !!! ###\n",
        "\n",
        "### Please put the structure definition of your model here\n",
        "\n",
        "PATH = 'checkpoint_best_loss.pth.tar'\n",
        "test_model = YourModel(*args, **kwargs)\n",
        "test_model.load_state_dict(torch.load(PATH)['state_dict'])\n",
        "test_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf_BhdWZqCQ5"
      },
      "source": [
        "### *(Do not modify any of the code below this section)*\n",
        "\n",
        "**Make sure your model has ``compress`` method like the one in the example network**\n",
        "\n",
        "please make sure your model can get through this benchmark, in which case it prints ``Final test result of your model``. \n",
        "\n",
        "Otherwise refer to the output for the guidance to adjust your network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3AwHuR5uB5p",
        "outputId": "00125825-21b7-42b8-e4a4-fc552a17eefa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pthflops\n",
            "  Downloading pthflops-0.4.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pthflops) (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pthflops) (3.12.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pthflops) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pthflops) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pthflops) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pthflops) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pthflops) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pthflops) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pthflops) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pthflops) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pthflops) (1.3.0)\n",
            "Installing collected packages: pthflops\n",
            "Successfully installed pthflops-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pthflops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ebQJhj-rfYFP"
      },
      "outputs": [],
      "source": [
        "# ################################################\n",
        "# import packages\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uAjGVoVnl1ZK"
      },
      "outputs": [],
      "source": [
        "seed = 123                                        # for reproducibility\n",
        "cuda = True                                       # use GPU\n",
        "save = True                                       # save trained model\n",
        "image_dataset = '/Your-path/ELEC5306/train'  # path to the root of the image dataset\n",
        "sequence_dataset = '/Your-path/ELEC5306/Video'  # path to the root of the video dataset\n",
        "checkpoint = ''                                   # load pretrained model\n",
        "epochs = 10                                       # total training epochs\n",
        "clip_max_norm = 1.0                               # avoid gradient explosion\n",
        "patch_size = (256, 256)                           # input size for the training network\n",
        "learning_rate = 1e-4  \n",
        "batch_size = 16\n",
        "test_batch_size = 16 \n",
        "num_workers = batch_size                          # multi-process for loading training data \n",
        "N = 128\n",
        "M = 192\n",
        "\n",
        "# ################################################\n",
        "# SequenceFolder Dataset\n",
        "\n",
        "class SequenceFolder(Dataset):\n",
        "    \"\"\"Load an image folder database. Training and testing image samples\n",
        "    are respectively stored in separate directories:\n",
        "\n",
        "    .. code-block::\n",
        "\n",
        "        - rootdir/\n",
        "            - train/\n",
        "              - video 1\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "              - video 2\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "            - test/\n",
        "              - video 1\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "              - video 2\n",
        "                - img000.png\n",
        "                - img001.png\n",
        "                ...\n",
        "\n",
        "    Args:\n",
        "        root (string): root directory of the dataset\n",
        "        transform (callable, optional): a function or transform that takes in a\n",
        "            PIL image and returns a transformed version\n",
        "        split (string): split mode ('train' or 'val')\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None, split=\"train\"):\n",
        "        self.mode = split\n",
        "        splitdir = Path(root) / split\n",
        "\n",
        "        if not splitdir.is_dir():\n",
        "            raise RuntimeError(f'Invalid directory \"{root}\"')\n",
        "\n",
        "        self.samples = self.get_all_images(splitdir)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def get_all_images(self, direc):\n",
        "        self.images = []\n",
        "        self.image_sequence = []\n",
        "        self.sequences = [f for f in direc.iterdir() if f.is_dir()]\n",
        "        for sd in self.sequences:\n",
        "            images = []\n",
        "            for f in (sd / 'img').iterdir():\n",
        "                if f.is_file():\n",
        "                  images.append(f)\n",
        "            self.image_sequence.append(images)\n",
        "            self.images = self.images + list(sorted(images))\n",
        "\n",
        "        return self.images\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            img: `PIL.Image.Image` or transformed `PIL.Image.Image`.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.samples[index]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            return self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "test_dataset = SequenceFolder(sequence_dataset, split=\"test\", transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "63X35flPm5fx"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Compute running average.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def PSNR(img1, img2):\n",
        "    # img1 and img2 within range [0, 1]\n",
        "    # img1 shape: (B, C, H, W)\n",
        "    # img2 shape: (B, C, H, W)\n",
        "    \n",
        "    img1, img2 = img1.detach(), img2.detach()\n",
        "    img1 = img1 * 255\n",
        "    img2 = img2 * 255\n",
        "    batch_size = img1.shape[0]\n",
        "    img1 = img1.reshape(batch_size, -1)\n",
        "    img2 = img2.reshape(batch_size, -1)\n",
        "    mse = torch.mean((img1 - img2) ** 2)\n",
        "    return torch.mean(20 * torch.log10(255.0 / torch.sqrt(mse)))\n",
        "\n",
        "def final_test_epoch(test_dataset, transform, model, criterion):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    mse_loss = AverageMeter()\n",
        "    psnr = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for video in test_dataset.image_sequence:\n",
        "            psnr_video = AverageMeter()\n",
        "            for image in video:\n",
        "                image = Image.open(image).convert(\"RGB\")\n",
        "                image = transform(image)\n",
        "                d = image.to(device).unsqueeze(0)\n",
        "                out_net = model(d)\n",
        "                d_out = out_net['x_hat']\n",
        "                out_criterion = criterion(out_net, d)\n",
        "                psnr_video.update(PSNR(d, d_out))\n",
        "\n",
        "            mse_loss.update(out_criterion[\"mse_loss\"])\n",
        "            psnr.update(psnr_video.avg)\n",
        "\n",
        "    print(\n",
        "        f\"Final test of model: Average losses:\"\n",
        "        f\"\\tMSE loss: {mse_loss.avg:.3f}\"\n",
        "        f'\\tSequence-wise PSNR: {psnr.avg: .3f}\\n'\n",
        "    )\n",
        "\n",
        "    return mse_loss.avg\n",
        "\n",
        "class Loss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        out = {}\n",
        "        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n",
        "        out[\"loss\"] = out[\"mse_loss\"] * 255\n",
        "\n",
        "        return out\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [transforms.CenterCrop(patch_size), transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "criterion = Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "k63OH5MEuS4V"
      },
      "outputs": [],
      "source": [
        "from pthflops import count_ops\n",
        "\n",
        "MAX_GFLOPS = 3000\n",
        "MIN_RATIO = 0.7\n",
        "\n",
        "def benchmark(yournet):\n",
        "    yournet = yournet.cuda()\n",
        "    x = torch.randn((1, 3, 500, 500)).cuda()\n",
        "    try:\n",
        "      yournet.compress(x)\n",
        "    except AttributeError:\n",
        "      print('does your network have a ```def compress(self, input)``` function?')\n",
        "      print('refer to the baseline model in the template')\n",
        "      return\n",
        "\n",
        "    est, _ = count_ops(yournet, x, print_readable=False, verbose=False)\n",
        "    est = int(est / 1e9)\n",
        "    if est <= MAX_GFLOPS:\n",
        "      print('#' * 30)\n",
        "      print('[Acceptable Model Complexity]')\n",
        "      print('#' * 30)\n",
        "      print('\\n')\n",
        "    else:\n",
        "      assert 0, 'Your model complexity is {} GFLOPS, the acceptable maximum is {} GFLOPS. Make your model smaller'.format(est, MAX_GFLOPS)\n",
        "\n",
        "    compressed = yournet.compress(x)\n",
        "    ratio = 1 - compressed.numel() / x.numel()\n",
        "    if ratio >= MIN_RATIO:\n",
        "      print('#' * 30)\n",
        "      print('[Acceptable Compression Ratio]')\n",
        "      print('#' * 30)\n",
        "    else:\n",
        "      assert 0, 'Current compression ratio is {} , the acceptable lowest ratio is {}. Make it higher'.format(ratio, MIN_RATIO)\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "    print('#' * 30)\n",
        "    print('Final test result of your model')\n",
        "    print('#' * 30)\n",
        "    print('\\n')\n",
        "\n",
        "    loss = final_test_epoch(test_dataset, test_transforms, yournet, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the below code if you need to upload the model weight file from your local PC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "1zEm8wods-Ps",
        "outputId": "d6423de4-f2ab-4f59-ea34-22cd23236cac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-28b2e496-37c8-442a-afbd-589973a841b7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-28b2e496-37c8-442a-afbd-589973a841b7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving checkpoint_best_loss.pth.tar to checkpoint_best_loss.pth.tar\n",
            "Uploaded checkpoint_best_loss.pth.tar\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "model_weight = files.upload()\n",
        "filename = next(iter(model_weight))\n",
        "print(\"Uploaded \" + filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCVcmff2kb3K",
        "outputId": "5b24a516-1517-4bcb-c074-041ecef365e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##############################\n",
            "[Acceptable Model Complexity]\n",
            "##############################\n",
            "\n",
            "\n",
            "##############################\n",
            "[Acceptable Compression Ratio]\n",
            "##############################\n",
            "\n",
            "\n",
            "##############################\n",
            "Final test result of your model\n",
            "##############################\n",
            "\n",
            "\n",
            "Final test of model: Average losses:\tMSE loss: 0.028\tSequence-wise PSNR:  16.474\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# an EXAMPLE network to run the test\n",
        "# test_model = Network(N, M)\n",
        "# test_model.load_state_dict(torch.load('checkpoint_best_loss.pth.tar')['state_dict'])\n",
        "# test_model.eval()\n",
        "benchmark(test_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
