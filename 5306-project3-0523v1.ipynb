{"cells":[{"cell_type":"markdown","metadata":{"id":"iUmYpQUBHN1W"},"source":["Project 3 Part B: A Deep Learning Toy Model for Video Compression\n","===\n","\n","In this project, we will use simple deep learning networks for basic video compression.\n","\n","For this project, we are going to use a template that's mostly from project 2. The reason is that a video is essentially a sequence of images, in which case a network for image compression is much likely to be re-usable for video compression.\n","\n","---\n","\n","## 1. Objectives, Requirements and Others\n","\n","### 1.1 Objectives\n","\n","* Customize your model and training process for better performance -- High PSNR on test videos.\n","* Make sure the model complexity not too high and compression ratio not too low.\n","\n","### 1.2 Requirements\n","\n","* Project 3 is an **Group Project**. The report should include the team member contributions\n","* **Training data** can only be the data under `Image` and `Video/train` folders that provided.\n","* Model limit: there's a **benchmark test** at the end of this notebook. It includes model complexity test and compression ratio test. Please **test your model with this and make sure it satisfies the requirement both in complexity and compression ratio**.\n","* **NO pre-trained model**. You are supposed to train the network from scratch.\n","\n","### 1.3 Submission\n","\n","Each student is supposed to upload a **{YOUR_FIRST_NAME}_{YOUR_UNIKEY}.zip** (e.g. Sydney_haha2333.zip) file with three files:\n","\n","* **a jupyternotebook** with the complete training and testing details of your final model. In other words, it is expected to reproduce the complete experiment process and reproduce the result on ```test``` datatest of your final model.\n","* **a .pth file** contains the trained weight of your model.\n","* **a PDF report** contains at least the following section:\n","  * Introduction\n","  * Method\n","  * Experiment & Analysis\n","  * Conclusion\n","  * (Further details are strongly encouraged, especially experiment analysis and the novelty of the method)\n","  * (Tables and Figures are encouraged)\n","\n","### 1.4 Marking Scheme\n","* Code -- 25%\n","  * the submitted code could run without error (except the path to the dataset) -- 15%\n","  * the complete experimental output and log of each code cell -- 5%\n","  * proper comment that makes the code easy to read -- 5%\n","\n","* PDF report -- 45%\n","  * well-written Introduction -- 5%\n","  * detailed discription of the method (e.g. includes an overview of the network structure of your method) -- 15%\n","  * detailed in-depth **quantitative** and **qualitative** analysis (e.g. tables and figures of results, how you go from baseline to the final model step-by-step and how each step affects the performance) -- 15%\n","  * good performance of the model -- 10%\n","\n","* Presentation -- 30%\n","  * easy-to-follow presentation -- 15%\n","  * detailed in-depth **quantitative** and **qualitative** analysis -- 15%\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Dzs2kueOgxN3"},"source":["---\n","\n","## 2. Preparation for the experiments"]},{"cell_type":"markdown","metadata":{"id":"VkzEhhMfA-_b"},"source":["### 2.1 Prepare the data\n","\n","Please refer to the data provided in Assignment 2\n","\n","**NOTE:**\n","\n","1. The image data part is exactly the same training data used for project 2\n","2. **ONLY data in image and video/train can be used for training**, no other sources allowed.\n","\n","Step-by-step\n","\n","1. Put those folder (and the data inside of course) in your GoogleDrive.\n","2. Run the command below to mount the GoogleDrive onto Colab.\n","3. Check the path to the data (e.g. `'/content/drive/MyDrive/MYDATA'`), and we will use this later.\n","\n","Hint: you can check the file with either bash command or use the leftmost sidebar in Colab, there's a folder shaped icon."]},{"cell_type":"markdown","metadata":{"id":"63Rv7d8v8O7_"},"source":["You can use drive.mount() to mount your Google Drive to the Colab file. For other usages, please refer to previous Ed posts and announcements."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-23T02:13:43.520073Z","iopub.status.busy":"2024-05-23T02:13:43.519346Z","iopub.status.idle":"2024-05-23T02:13:43.524024Z","shell.execute_reply":"2024-05-23T02:13:43.523022Z","shell.execute_reply.started":"2024-05-23T02:13:43.520039Z"},"executionInfo":{"elapsed":19606,"status":"ok","timestamp":1716009044926,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"6bgKCJIe9jZh","outputId":"5cdd4df6-5b67-4229-b85c-5ad61efd7ea9","trusted":true},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"2GC-BrL3kmoN"},"source":["### 2.2 Preliminary: Conversion between images and videos\n","\n","A video is essentially a sequence of consecutive images. We can convert a video to images and vice versa. Here is an example."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:13:43.527466Z","iopub.status.busy":"2024-05-23T02:13:43.527090Z","iopub.status.idle":"2024-05-23T02:13:50.194076Z","shell.execute_reply":"2024-05-23T02:13:50.193023Z","shell.execute_reply.started":"2024-05-23T02:13:43.527435Z"},"trusted":true},"outputs":[],"source":["# ################################################\n","# import packages\n","\n","import argparse\n","import time\n","import math\n","import random\n","import shutil\n","import sys\n","import glob\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","\n","from pathlib import Path\n","\n","from PIL import Image\n","from torch.utils.data import Dataset\n","\n","\n","import os\n","import cv2\n","import glob\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","global_path = '/home/harryk/usyd/elec5306/imageset'"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-23T02:13:50.196356Z","iopub.status.busy":"2024-05-23T02:13:50.195937Z","iopub.status.idle":"2024-05-23T02:13:50.246074Z","shell.execute_reply":"2024-05-23T02:13:50.244918Z","shell.execute_reply.started":"2024-05-23T02:13:50.196328Z"},"executionInfo":{"elapsed":530,"status":"ok","timestamp":1716009045454,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"pYn0j9DqlZxM","outputId":"99574505-650a-414f-bbf9-994d3bc42172","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","success\n"]}],"source":["# convert image sequence to a video\n","\n","\n","\n","path_to_sequence = os.path.join(global_path, 'Video/train/Bird2')\n","img_array = []\n","print('True' if os.path.exists(path_to_sequence) else 'False')\n","size = (0,0)\n","\n","# remember to sort the images before conversion,\n","# otherwise they might be in wrong temporal order\n","for filename in sorted(glob.glob(os.path.join(path_to_sequence, '*.jpg'))):\n","    img = cv2.imread(filename)\n","    height, width, layers = img.shape\n","    size = (width,height)\n","    img_array.append(img)\n","\n","# use mp4v instead of DIVX\n","out = cv2.VideoWriter(os.path.join(path_to_sequence, 'video.mp4'),cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n","# out = cv2.VideoWriter('/Users/harryk/Documents/usyd/2024sem1/ELEC5306/Project3/imageset/Video/train/Bird2/video.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n","\n","for i in range(len(img_array)):\n","    out.write(img_array[i])\n","out.release()\n","print('success' if os.path.exists(os.path.join(path_to_sequence, 'video.mp4')) else 'failed')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-23T02:13:50.247617Z","iopub.status.busy":"2024-05-23T02:13:50.247322Z","iopub.status.idle":"2024-05-23T02:13:50.284406Z","shell.execute_reply":"2024-05-23T02:13:50.283472Z","shell.execute_reply.started":"2024-05-23T02:13:50.247591Z"},"executionInfo":{"elapsed":23188,"status":"ok","timestamp":1716009068641,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"Or76XY_orfou","outputId":"a7c3b07a-dd01-4070-cba6-09f0201db572","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: Could not open video stream from /home/harryk/usyd/elec5306/imageset/Video/train/Bird2/video.mp4\n"]}],"source":["# convert video to image sequence\n","\n","def getFrame(vid_path, out_dir):\n","    if not os.path.exists(vid_path):\n","        print(f\"Error: The video file {vid_path} does not exist.\")\n","        return\n","    vidcap = cv2.VideoCapture(vid_path)\n","    if not vidcap.isOpened():\n","        print(f\"Error: Could not open video stream from {vid_path}\")\n","        return\n","    hasFrames = True\n","    count = 0\n","    while hasFrames:\n","      hasFrames,image = vidcap.read()\n","      if hasFrames:\n","        cv2.imwrite(os.path.join(out_dir, \"image_\"+str(count)+\".jpg\"), image)     # save frame as JPG file\n","        count += 1\n","      else:\n","        print('end of sequence')\n","        break\n","\n","getFrame(\n","   os.path.join(global_path, 'Video/train/Bird2/video.mp4'),\n","    os.path.join(global_path, 'Video/train/Bird2/img_from_video')\n",")"]},{"cell_type":"markdown","metadata":{"id":"aAsWk6l-xuim"},"source":["### 2.3 Get GPU ready\n","\n","Change Runtime Type in the Colab menu to 'GPU'."]},{"cell_type":"markdown","metadata":{"id":"Byw0cexcCqiM"},"source":["---\n","\n","## 3. Code Template\n","\n","Here is a template for training and testing a baseline model. The model is the same one we use for the Project 2. The training process are the same, while the testing are performed on video sequences."]},{"cell_type":"markdown","metadata":{"id":"xDgQqbjXeTfn"},"source":["Let's firt import packages for this project."]},{"cell_type":"markdown","metadata":{"id":"owR0dmNvfQmW"},"source":["Then we define a Dataset class that could iterate over images under a directory."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:13:50.287452Z","iopub.status.busy":"2024-05-23T02:13:50.287104Z","iopub.status.idle":"2024-05-23T02:13:50.298252Z","shell.execute_reply":"2024-05-23T02:13:50.297104Z","shell.execute_reply.started":"2024-05-23T02:13:50.287423Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716009075335,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"kUZuUOkVE6Qw","trusted":true},"outputs":[],"source":["# ################################################\n","# ImageFolder Dataset\n","\n","class ImageFolder(Dataset):\n","    \"\"\"Load an image folder database. Training and testing image samples\n","    are respectively stored in separate directories:\n","\n","    .. code-block::\n","\n","        - rootdir/\n","            - train/\n","                - img000.png\n","                - img001.png\n","                ...\n","            - valid/\n","                - img000.png\n","                - img001.png\n","                ...\n","\n","    Args:\n","        root (string): root directory of the dataset\n","        transform (callable, optional): a function or transform that takes in a\n","            PIL image and returns a transformed version\n","    \"\"\"\n","\n","    def __init__(self, root, transform=None):\n","        splitdir = Path(root)\n","\n","        if not splitdir.is_dir():\n","            raise RuntimeError(f'Invalid directory \"{root}\"')\n","\n","        self.samples = [f for f in splitdir.iterdir() if f.is_file()]\n","\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","\n","        Returns:\n","            img: `PIL.Image.Image` or transformed `PIL.Image.Image`.\n","        \"\"\"\n","        img = Image.open(self.samples[index]).convert(\"RGB\")\n","        if self.transform:\n","            return self.transform(img)\n","        return img\n","\n","    def __len__(self):\n","        return len(self.samples)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:13:50.300063Z","iopub.status.busy":"2024-05-23T02:13:50.299700Z","iopub.status.idle":"2024-05-23T02:13:50.315670Z","shell.execute_reply":"2024-05-23T02:13:50.314695Z","shell.execute_reply.started":"2024-05-23T02:13:50.300032Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716009075335,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"lBKEXKmjChPU","trusted":true},"outputs":[],"source":["# ################################################\n","# SequenceFolder Dataset\n","\n","class SequenceFolder(Dataset):\n","    \"\"\"Load an image folder database. Training and testing image samples\n","    are respectively stored in separate directories:\n","\n","    .. code-block::\n","\n","        - rootdir/\n","            - train/\n","              - video 1\n","                - img000.png\n","                - img001.png\n","                ...\n","              - video 2\n","                - img000.png\n","                - img001.png\n","                ...\n","            - test/\n","              - video 1\n","                - img000.png\n","                - img001.png\n","                ...\n","              - video 2\n","                - img000.png\n","                - img001.png\n","                ...\n","\n","    Args:\n","        root (string): root directory of the dataset\n","        transform (callable, optional): a function or transform that takes in a\n","            PIL image and returns a transformed version\n","        split (string): split mode ('train' or 'val')\n","    \"\"\"\n","\n","    def __init__(self, root, transform=None, split=\"train\"):\n","        self.mode = split\n","        splitdir = Path(root) / split\n","\n","        if not splitdir.is_dir():\n","            raise RuntimeError(f'Invalid directory \"{root}\"')\n","\n","        self.samples = self.get_all_images(splitdir)\n","\n","        self.transform = transform\n","\n","    def get_all_images(self, direc):\n","        self.images = []\n","        self.image_sequence = []\n","        self.sequences = [f for f in direc.iterdir() if f.is_dir()]\n","        for sd in self.sequences:\n","            images = []\n","            for f in (sd / 'img').iterdir():\n","                if f.is_file():\n","                  images.append(f)\n","            self.image_sequence.append(images)\n","            self.images = self.images + list(sorted(images))\n","\n","        return self.images\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","\n","        Returns:\n","            img: `PIL.Image.Image` or transformed `PIL.Image.Image`.\n","        \"\"\"\n","        img = Image.open(self.samples[index]).convert(\"RGB\")\n","        if self.transform:\n","            return self.transform(img)\n","        return img\n","\n","    def __len__(self):\n","        return len(self.samples)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-23T02:13:50.317345Z","iopub.status.busy":"2024-05-23T02:13:50.316983Z","iopub.status.idle":"2024-05-23T02:13:50.443858Z","shell.execute_reply":"2024-05-23T02:13:50.442922Z","shell.execute_reply.started":"2024-05-23T02:13:50.317314Z"},"executionInfo":{"elapsed":1377,"status":"ok","timestamp":1716009076710,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"2HtrHCpgFCeK","outputId":"64a90d42-87e4-44f7-b426-31f83ad0ad55","trusted":true},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["d = SequenceFolder(global_path)\n","d.image_sequence"]},{"cell_type":"markdown","metadata":{"id":"Kq1tuazucdCr"},"source":["Then let's define a baseline neural network.\n","\n","**NOTE:**\n","\n","As in the requirement, please use the ```benchmark``` function at the end of this template to check your model's complexity and compression ratio, and see if both meet the requirement."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:13:50.445548Z","iopub.status.busy":"2024-05-23T02:13:50.445186Z","iopub.status.idle":"2024-05-23T02:13:50.458464Z","shell.execute_reply":"2024-05-23T02:13:50.457538Z","shell.execute_reply.started":"2024-05-23T02:13:50.445516Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716009076710,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"jIDZbA0JE8cQ","trusted":true},"outputs":[],"source":["# ###################################################\n","# a baseline model\n","\n","def conv(in_channels, out_channels, kernel_size=5, stride=2):\n","    return nn.Conv2d(\n","        in_channels,\n","        out_channels,\n","        kernel_size=kernel_size,\n","        stride=stride,\n","        padding=kernel_size // 2,\n","    )\n","\n","\n","def deconv(in_channels, out_channels, kernel_size=5, stride=2):\n","    return nn.ConvTranspose2d(\n","        in_channels,\n","        out_channels,\n","        kernel_size=kernel_size,\n","        stride=stride,\n","        output_padding=stride - 1,\n","        padding=kernel_size // 2,\n","    )\n","\n","\n","class Network(nn.Module):\n","\n","    def __init__(self, N, M, init_weights=True, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.g_a = nn.Sequential(\n","            conv(3, N),\n","            conv(N, N),\n","            conv(N, N),\n","            conv(N, M),\n","        )\n","\n","        self.g_s = nn.Sequential(\n","            deconv(M, N),\n","            deconv(N, N),\n","            deconv(N, N),\n","            deconv(N, 3),\n","        )\n","\n","        self.N = N\n","        self.M = M\n","\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        y = self.g_a(x)\n","        x_hat = self.g_s(y)\n","\n","        return {\n","            \"x_hat\": x_hat,\n","        }\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","                nn.init.kaiming_normal_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","\n","    def compress(self, x):\n","        y = self.g_a(x)\n","        return y\n","\n","    def decompress(self, y_hat):\n","        x_hat = self.g_s(y_hat).clamp_(0, 1)\n","        return {\"x_hat\": x_hat}\n"]},{"cell_type":"markdown","metadata":{},"source":["### Network model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.171247Z","iopub.status.busy":"2024-05-23T02:18:31.170774Z","iopub.status.idle":"2024-05-23T02:18:31.187210Z","shell.execute_reply":"2024-05-23T02:18:31.186381Z","shell.execute_reply.started":"2024-05-23T02:18:31.171214Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Encoder(nn.Module):\n","    def __init__(self, N=128, M=192):\n","        super(Encoder, self).__init__()\n","        self.conv1 = nn.Conv2d(3, N, kernel_size=3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(N, N * 2, kernel_size=3, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(N * 2, N * 2, kernel_size=3, stride=2, padding=1)\n","        self.conv4 = nn.Conv2d(N * 2, M, kernel_size=3, stride=2, padding=1)\n","        self.bn1 = nn.BatchNorm2d(N)\n","        self.act = nn.LeakyReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x1 = self.act(self.bn1(self.conv1(x)))\n","        x2 = self.act(self.conv2(x1))\n","        x3 = self.act(self.conv3(x2))\n","        x4 = self.act(self.conv4(x3))\n","        return x4, [x1, x2, x3]\n","\n","class Decoder(nn.Module):\n","    def __init__(self, N=128, M=192):\n","        super(Decoder, self).__init__()\n","        self.deconv1 = nn.ConvTranspose2d(M, N * 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n","        self.deconv2 = nn.ConvTranspose2d(N * 4, N * 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n","        self.deconv3 = nn.ConvTranspose2d(N * 4, N, kernel_size=3, stride=2, padding=1, output_padding=1)\n","        self.deconv4 = nn.ConvTranspose2d(N * 2, 3, kernel_size=3, stride=2, padding=1, output_padding=1)\n","        self.bn1 = nn.BatchNorm2d(N * 2)\n","        self.act = nn.LeakyReLU(inplace=True)\n","\n","    def forward(self, x, skips):\n","        x = self.act(self.bn1(self.deconv1(x)))\n","        x = torch.cat((x, skips[2]), dim=1)\n","        x = self.act(self.deconv2(x))\n","        x = torch.cat((x, skips[1]), dim=1)\n","        x = self.act(self.deconv3(x))\n","        x = torch.cat((x, skips[0]), dim=1)\n","        x = torch.tanh(self.deconv4(x))\n","        return x\n","\n","class CompressionNetwork(nn.Module):\n","    def __init__(self, N=128, M=192):\n","        super(CompressionNetwork, self).__init__()\n","        self.encoder = Encoder(N, M)\n","        self.decoder = Decoder(N, M)\n","\n","    def forward(self, x):\n","        if x.dim() < 4:\n","            x = x.unsqueeze(0)\n","        y, skips = self.encoder(x)\n","        x_hat = self.decoder(y, skips)\n","        return {\"x_hat\": x_hat}\n","\n","    def compress(self, x):\n","        y, _ = self.encoder(x)\n","        return y\n","\n","    def decompress(self, y):\n","        x_hat = self.decoder(y, [None, None, None]).clamp_(0, 1)\n","        return {\"x_hat\": x_hat}\n"]},{"cell_type":"markdown","metadata":{"id":"Yd9e3NRtlgi2"},"source":["Here are some other ingredients for training and testing."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.189344Z","iopub.status.busy":"2024-05-23T02:18:31.189056Z","iopub.status.idle":"2024-05-23T02:18:31.204891Z","shell.execute_reply":"2024-05-23T02:18:31.204023Z","shell.execute_reply.started":"2024-05-23T02:18:31.189321Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716009076710,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"pRlDgjicFtdN","trusted":true},"outputs":[],"source":["\n","\n","\n","class Loss(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, output, target):\n","        out = {}\n","        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n","        out[\"loss\"] = out[\"mse_loss\"] * 255\n","\n","        return out"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.206548Z","iopub.status.busy":"2024-05-23T02:18:31.206201Z","iopub.status.idle":"2024-05-23T02:18:31.216356Z","shell.execute_reply":"2024-05-23T02:18:31.215518Z","shell.execute_reply.started":"2024-05-23T02:18:31.206518Z"},"executionInfo":{"elapsed":428,"status":"ok","timestamp":1716009077137,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"MF1C9J_NYYoe","trusted":true},"outputs":[],"source":["class AverageMeter:\n","    \"\"\"Compute running average.\"\"\"\n","\n","    def __init__(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.219267Z","iopub.status.busy":"2024-05-23T02:18:31.218590Z","iopub.status.idle":"2024-05-23T02:18:31.226886Z","shell.execute_reply":"2024-05-23T02:18:31.225930Z","shell.execute_reply.started":"2024-05-23T02:18:31.219236Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716009077137,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"rwmPi5mebRu8","trusted":true},"outputs":[],"source":["def configure_optimizers(net, learning_rate):\n","\n","    optimizer = optim.Adam(\n","        net.parameters(),\n","        lr=learning_rate,\n","    )\n","\n","    return optimizer"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.228389Z","iopub.status.busy":"2024-05-23T02:18:31.228052Z","iopub.status.idle":"2024-05-23T02:18:31.241451Z","shell.execute_reply":"2024-05-23T02:18:31.240544Z","shell.execute_reply.started":"2024-05-23T02:18:31.228358Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1716009077137,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"-rJt2-91bv4c","trusted":true},"outputs":[],"source":["def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n","    torch.save(state, filename)\n","    if is_best:\n","        print('Saving best model...')\n","        shutil.copyfile(filename, \"checkpoint_best_loss.pth.tar\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.243010Z","iopub.status.busy":"2024-05-23T02:18:31.242689Z","iopub.status.idle":"2024-05-23T02:18:31.252997Z","shell.execute_reply":"2024-05-23T02:18:31.252101Z","shell.execute_reply.started":"2024-05-23T02:18:31.242985Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1716009077137,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"rKc3sUFXPK5d","trusted":true},"outputs":[],"source":["def PSNR(img1, img2):\n","    # img1 and img2 within range [0, 1]\n","    # img1 shape: (B, C, H, W)\n","    # img2 shape: (B, C, H, W)\n","\n","    img1, img2 = img1.detach(), img2.detach()\n","    img1 = img1 * 255\n","    img2 = img2 * 255\n","    batch_size = img1.shape[0]\n","    img1 = img1.reshape(batch_size, -1)\n","    img2 = img2.reshape(batch_size, -1)\n","    mse = torch.mean((img1 - img2) ** 2)\n","    return torch.mean(20 * torch.log10(255.0 / torch.sqrt(mse)))"]},{"cell_type":"markdown","metadata":{"id":"cCZLieuUjIn5"},"source":["After we almost got everything we need, we define a `train_one_epoch` function for training the model for one epoch."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.254503Z","iopub.status.busy":"2024-05-23T02:18:31.254170Z","iopub.status.idle":"2024-05-23T02:18:31.265905Z","shell.execute_reply":"2024-05-23T02:18:31.264998Z","shell.execute_reply.started":"2024-05-23T02:18:31.254472Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716009077137,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"M-J38tMfbiFR","trusted":true},"outputs":[],"source":["def train_one_epoch(\n","    model, criterion, train_dataloader, optimizer, epoch, clip_max_norm\n","):\n","    model.train()\n","    device = next(model.parameters()).device\n","\n","    for i, d in enumerate(train_dataloader):\n","        d = d.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        out_net = model(d)\n","\n","        out_criterion = criterion(out_net, d)\n","        out_criterion[\"loss\"].backward()\n","        if clip_max_norm > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n","        optimizer.step()\n","\n","        if i % 20 == 0:\n","            print(\n","                f\"Train epoch {epoch}: [\"\n","                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n","                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n","                f'\\tLoss: {out_criterion[\"loss\"].item():.3f} |'\n","                f'\\tMSE loss: {out_criterion[\"mse_loss\"].item():.3f}'\n","            )\n"]},{"cell_type":"markdown","metadata":{"id":"J61e5WBajaz7"},"source":["Likewise we further define a `test_epoch` function.\n","\n","Unlike project 2, this time the metric is measured on videos instead of images."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.302243Z","iopub.status.busy":"2024-05-23T02:18:31.301884Z","iopub.status.idle":"2024-05-23T02:18:31.315648Z","shell.execute_reply":"2024-05-23T02:18:31.314616Z","shell.execute_reply.started":"2024-05-23T02:18:31.302213Z"},"executionInfo":{"elapsed":276,"status":"ok","timestamp":1716010689041,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"kdTl131jbqnX","trusted":true},"outputs":[],"source":["def test_epoch(epoch, test_dataset, transform, model, criterion):\n","    model.eval()\n","    device = next(model.parameters()).device\n","\n","    mse_loss = AverageMeter()\n","    psnr = AverageMeter()\n","\n","    with torch.no_grad():\n","        for i, video in enumerate(test_dataset.image_sequence):\n","            psnr_video = AverageMeter()\n","            for j, image in enumerate(video):\n","                image = Image.open(image).convert(\"RGB\")\n","                image = transform(image)\n","                d = image.to(device).unsqueeze(0)\n","                out_net = model(d)\n","                d_out = out_net['x_hat']\n","                out_criterion = criterion(out_net, d)\n","                psnr_video.update(PSNR(d, d_out))\n","\n","            mse_loss.update(out_criterion[\"mse_loss\"])\n","            psnr.update(psnr_video.avg)\n","\n","    print(\n","        f\"Test epoch {epoch}: Average losses:\"\n","        f\"\\tMSE loss: {mse_loss.avg:.3f}\"\n","        f'\\tSequence-wise PSNR: {psnr.avg: .3f}\\n'\n","    )\n","\n","    return mse_loss.avg\n","\n","def test_epoch(epoch, test_dataset, model, criterion, batch_size=256):\n","    model.eval()\n","    device = next(model.parameters()).device\n","\n","    mse_loss = AverageMeter()\n","    psnr = AverageMeter()\n","\n","    data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            psnr_video = AverageMeter()\n","            for images in batch:\n","                images = images.to(device)\n","                out_net = model(images)\n","                d_out = out_net['x_hat']\n","                out_criterion = criterion(out_net, images)\n","                psnr_video.update(PSNR(images, d_out))\n","\n","            mse_loss.update(out_criterion[\"mse_loss\"])\n","            psnr.update(psnr_video.avg)\n","\n","    print(\n","        f\"Test epoch {epoch}: Average losses:\"\n","        f\"\\tMSE loss: {mse_loss.avg:.3f}\"\n","        f'\\tSequence-wise PSNR: {psnr.avg: .3f}\\n'\n","    )\n","\n","    return mse_loss.avg"]},{"cell_type":"markdown","metadata":{"id":"X9KqN_qlkDB0"},"source":["Finally we define a main function, and complete the whole training/testing process.\n","\n","At the start of this main function, we define some arguments that's useful for training."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:18:31.317620Z","iopub.status.busy":"2024-05-23T02:18:31.317338Z","iopub.status.idle":"2024-05-23T02:18:32.291627Z","shell.execute_reply":"2024-05-23T02:18:32.290506Z","shell.execute_reply.started":"2024-05-23T02:18:31.317596Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716009077137,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"x63ecQUskvZ7","trusted":true},"outputs":[],"source":["# !ls /Your-path/ELEC5306"]},{"cell_type":"markdown","metadata":{"id":"Wk512yTjv7Lh"},"source":["Define some arguments useful for training/validation."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T02:28:55.740356Z","iopub.status.busy":"2024-05-23T02:28:55.739981Z","iopub.status.idle":"2024-05-23T02:28:55.747274Z","shell.execute_reply":"2024-05-23T02:28:55.746089Z","shell.execute_reply.started":"2024-05-23T02:28:55.740328Z"},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1716010693630,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"-h-p6LA_bw99","trusted":true},"outputs":[],"source":["seed = 123                                        # for reproducibility\n","cuda = True                                       # use GPU\n","save = True                                       # save trained model\n","image_dataset = os.path.join(global_path,'train')  # path to the root of the image dataset\n","sequence_dataset = os.path.join(global_path,'Video')  # path to the root of the video dataset\n","checkpoint = ''                                   # load pretrained model\n","epochs = 50                                       # total training epochs\n","clip_max_norm = 1.0                               # avoid gradient explosion\n","patch_size = (96, 128)                           # input size for the training network\n","learning_rate = 3e-4\n","batch_size = 32\n","test_batch_size = 32\n","num_workers = 256                         # multi-process for loading training data\n","N = 128\n","M = 192"]},{"cell_type":"markdown","metadata":{},"source":["### Train"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-23T02:28:55.749544Z","iopub.status.busy":"2024-05-23T02:28:55.749265Z","iopub.status.idle":"2024-05-23T03:12:33.531778Z","shell.execute_reply":"2024-05-23T03:12:33.529687Z","shell.execute_reply.started":"2024-05-23T02:28:55.749521Z"},"executionInfo":{"elapsed":1321516,"status":"ok","timestamp":1716013239610,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"ZHEARHw8v56D","outputId":"2ffecd46-b005-4517-ad18-49445136e041","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/harryk/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:556: UserWarning: This DataLoader will create 256 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"name":"stdout","output_type":"stream","text":["Learning rate: 0.0003\n","Train epoch 0: [0/13590 (0%)]\tLoss: 234.625 |\tMSE loss: 0.920\n","Train epoch 0: [640/13590 (5%)]\tLoss: 29.266 |\tMSE loss: 0.115\n","Train epoch 0: [1280/13590 (9%)]\tLoss: 13.376 |\tMSE loss: 0.052\n","Train epoch 0: [1920/13590 (14%)]\tLoss: 9.596 |\tMSE loss: 0.038\n","Train epoch 0: [2560/13590 (19%)]\tLoss: 8.582 |\tMSE loss: 0.034\n","Train epoch 0: [3200/13590 (24%)]\tLoss: 8.088 |\tMSE loss: 0.032\n","Train epoch 0: [3840/13590 (28%)]\tLoss: 4.302 |\tMSE loss: 0.017\n","Train epoch 0: [4480/13590 (33%)]\tLoss: 4.095 |\tMSE loss: 0.016\n","Train epoch 0: [5120/13590 (38%)]\tLoss: 3.253 |\tMSE loss: 0.013\n","Train epoch 0: [5760/13590 (42%)]\tLoss: 2.899 |\tMSE loss: 0.011\n","Train epoch 0: [6400/13590 (47%)]\tLoss: 3.835 |\tMSE loss: 0.015\n","Train epoch 0: [7040/13590 (52%)]\tLoss: 2.352 |\tMSE loss: 0.009\n","Train epoch 0: [7680/13590 (56%)]\tLoss: 1.760 |\tMSE loss: 0.007\n","Train epoch 0: [8320/13590 (61%)]\tLoss: 3.437 |\tMSE loss: 0.013\n","Train epoch 0: [8960/13590 (66%)]\tLoss: 2.014 |\tMSE loss: 0.008\n","Train epoch 0: [9600/13590 (71%)]\tLoss: 1.907 |\tMSE loss: 0.007\n","Train epoch 0: [10240/13590 (75%)]\tLoss: 1.369 |\tMSE loss: 0.005\n","Train epoch 0: [10880/13590 (80%)]\tLoss: 2.101 |\tMSE loss: 0.008\n","Train epoch 0: [11520/13590 (85%)]\tLoss: 2.036 |\tMSE loss: 0.008\n","Train epoch 0: [12160/13590 (89%)]\tLoss: 1.276 |\tMSE loss: 0.005\n","Train epoch 0: [12800/13590 (94%)]\tLoss: 1.843 |\tMSE loss: 0.007\n","Train epoch 0: [13440/13590 (99%)]\tLoss: 1.288 |\tMSE loss: 0.005\n"]},{"name":"stderr","output_type":"stream","text":["/home/harryk/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([3, 96, 128])) that is different to the input size (torch.Size([1, 3, 96, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Test epoch 0: Average losses:\tMSE loss: 0.008\tSequence-wise PSNR:  21.561\n","\n","Saving best model...\n","Learning rate: 0.0003\n","Train epoch 1: [0/13590 (0%)]\tLoss: 1.549 |\tMSE loss: 0.006\n","Train epoch 1: [640/13590 (5%)]\tLoss: 1.199 |\tMSE loss: 0.005\n","Train epoch 1: [1280/13590 (9%)]\tLoss: 1.251 |\tMSE loss: 0.005\n","Train epoch 1: [1920/13590 (14%)]\tLoss: 1.454 |\tMSE loss: 0.006\n","Train epoch 1: [2560/13590 (19%)]\tLoss: 1.021 |\tMSE loss: 0.004\n","Train epoch 1: [3200/13590 (24%)]\tLoss: 2.222 |\tMSE loss: 0.009\n","Train epoch 1: [3840/13590 (28%)]\tLoss: 1.322 |\tMSE loss: 0.005\n","Train epoch 1: [4480/13590 (33%)]\tLoss: 1.514 |\tMSE loss: 0.006\n","Train epoch 1: [5120/13590 (38%)]\tLoss: 1.258 |\tMSE loss: 0.005\n","Train epoch 1: [5760/13590 (42%)]\tLoss: 1.871 |\tMSE loss: 0.007\n","Train epoch 1: [6400/13590 (47%)]\tLoss: 1.610 |\tMSE loss: 0.006\n","Train epoch 1: [7040/13590 (52%)]\tLoss: 1.523 |\tMSE loss: 0.006\n","Train epoch 1: [7680/13590 (56%)]\tLoss: 0.581 |\tMSE loss: 0.002\n","Train epoch 1: [8320/13590 (61%)]\tLoss: 2.593 |\tMSE loss: 0.010\n","Train epoch 1: [8960/13590 (66%)]\tLoss: 0.702 |\tMSE loss: 0.003\n","Train epoch 1: [9600/13590 (71%)]\tLoss: 0.673 |\tMSE loss: 0.003\n","Train epoch 1: [10240/13590 (75%)]\tLoss: 1.225 |\tMSE loss: 0.005\n","Train epoch 1: [10880/13590 (80%)]\tLoss: 1.038 |\tMSE loss: 0.004\n","Train epoch 1: [11520/13590 (85%)]\tLoss: 0.821 |\tMSE loss: 0.003\n","Train epoch 1: [12160/13590 (89%)]\tLoss: 0.785 |\tMSE loss: 0.003\n","Train epoch 1: [12800/13590 (94%)]\tLoss: 1.245 |\tMSE loss: 0.005\n","Train epoch 1: [13440/13590 (99%)]\tLoss: 0.783 |\tMSE loss: 0.003\n"]}],"source":["torch.manual_seed(seed)\n","random.seed(seed)\n","\n","train_transforms = transforms.Compose(\n","    [transforms.RandomCrop(patch_size), transforms.ToTensor()]\n",")\n","\n","test_transforms = transforms.Compose(\n","    [transforms.CenterCrop(patch_size), transforms.ToTensor()]\n",")\n","\n","# train_dataset = ImageFolder(image_dataset, transform=train_transforms)\n","train_dataset = SequenceFolder(sequence_dataset, split=\"train\", transform=train_transforms)\n","test_dataset = SequenceFolder(sequence_dataset, split=\"test\", transform=test_transforms)\n","\n","device = \"cuda\" if cuda and torch.cuda.is_available() else \"mps\"\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    num_workers=num_workers,\n","    shuffle=True,\n","    pin_memory=(device == \"cuda\" or \"mps\"),\n",")\n","\n","\n","\n","\n","\n","net = CompressionNetwork(N, M)\n","net = net.to(device)\n","\n","optimizer = configure_optimizers(net, learning_rate)\n","lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\",factor=0.5,patience=10)\n","criterion = Loss()\n","\n","last_epoch = 0\n","if checkpoint:  # load from previous checkpoint\n","    print(\"Loading\", checkpoint)\n","    checkpoint = torch.load(checkpoint, map_location=device)\n","    last_epoch = checkpoint[\"epoch\"] + 1\n","    net.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n","\n","best_loss = float(\"inf\")\n","train_time = AverageMeter()\n","for epoch in range(last_epoch, epochs):\n","    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n","    epoch_train_start = time.time()\n","    train_one_epoch(\n","        net,\n","        criterion,\n","        train_dataloader,\n","        optimizer,\n","        epoch,\n","        clip_max_norm,\n","    )\n","    epoch_train_end = time.time()\n","    train_time.update(epoch_train_end - epoch_train_start)\n","    loss = test_epoch(epoch, test_dataset, net, criterion)\n","    lr_scheduler.step(loss)\n","\n","    is_best = loss < best_loss\n","    best_loss = min(loss, best_loss)\n","\n","    if save:\n","      save_checkpoint(\n","          {\n","              \"epoch\": epoch,\n","              \"state_dict\": net.state_dict(),\n","              \"loss\": loss,\n","              \"optimizer\": optimizer.state_dict(),\n","              \"lr_scheduler\": lr_scheduler.state_dict(),\n","          },\n","          is_best,\n","      )\n","print('the overall training time (exclude testing) is {} min'.format(train_time.sum / 60))"]},{"cell_type":"markdown","metadata":{},"source":["# benchmark"]},{"cell_type":"markdown","metadata":{"id":"TxSwpCHlmNWV"},"source":["### Benchmark Test (VERY IMPORTANT)\n","\n","1. Please use 'torch.save(model.state_dict(), PATH)' to save your trained model\n","\n","2. Please use the below code block, to instantiate your model as object 'test_model' for marking. We will use 'load_state_dict()' to load your model. Please make sure your saved model can pass the loading test. You can use the below code block to test.\n","\n","3. Please include your trained model weight in the submission.\n","\n","4. **If we cannot load your uploaded weight, you will get zero mark for this assignment.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-05-23T03:12:33.532762Z","iopub.status.idle":"2024-05-23T03:12:33.533119Z","shell.execute_reply":"2024-05-23T03:12:33.532964Z","shell.execute_reply.started":"2024-05-23T03:12:33.532948Z"},"executionInfo":{"elapsed":519,"status":"ok","timestamp":1716013370492,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"5ry2cD21p_ms","outputId":"2796c76a-c66d-45b1-e5d5-7140a50ff144","trusted":true},"outputs":[{"data":{"text/plain":["Network(\n","  (g_a): Sequential(\n","    (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","    (1): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","    (2): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","    (3): Conv2d(128, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","  )\n","  (g_s): Sequential(\n","    (0): ConvTranspose2d(192, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n","    (1): ConvTranspose2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n","    (2): ConvTranspose2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n","    (3): ConvTranspose2d(128, 3, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n","  )\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["### EDIT REQUIRED !!! ###\n","\n","### Please put the structure definition of your model here\n","\n","PATH = 'checkpoint_best_loss.pth.tar'\n","test_model = Network(N,M)\n","# test_model.load_state_dict(torch.load(PATH)['state_dict'])\n","test_model.eval()"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"vf_BhdWZqCQ5"},"source":["### *(Do not modify any of the code below this section)*\n","\n","**Make sure your model has ``compress`` method like the one in the example network**\n","\n","please make sure your model can get through this benchmark, in which case it prints ``Final test result of your model``.\n","\n","Otherwise refer to the output for the guidance to adjust your network."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-05-23T03:12:33.534804Z","iopub.status.idle":"2024-05-23T03:12:33.535272Z","shell.execute_reply":"2024-05-23T03:12:33.535052Z","shell.execute_reply.started":"2024-05-23T03:12:33.535033Z"},"executionInfo":{"elapsed":72359,"status":"ok","timestamp":1716013488334,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"X3AwHuR5uB5p","outputId":"fc35d824-c744-4c10-94d4-385fccb819d6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pthflops in /home/harryk/.local/lib/python3.10/site-packages (0.4.2)\n","Requirement already satisfied: torch in /home/harryk/.local/lib/python3.10/site-packages (from pthflops) (2.4.0.dev20240517+cu121)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (11.4.5.107)\n","Requirement already satisfied: jinja2 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (12.1.105)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (2.20.5)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (11.0.2.54)\n","Requirement already satisfied: pytorch-triton==3.0.0+45fff310c8 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (3.0.0+45fff310c8)\n","Requirement already satisfied: fsspec in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (2024.2.0)\n","Requirement already satisfied: sympy in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (1.12)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (12.1.105)\n","Requirement already satisfied: filelock in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (3.13.1)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (12.1.0.106)\n","Requirement already satisfied: networkx in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (3.2.1)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (12.1.3.1)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (10.3.2.106)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (4.8.0)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/harryk/.local/lib/python3.10/site-packages (from torch->pthflops) (12.1.105)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/harryk/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->pthflops) (12.1.105)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/harryk/.local/lib/python3.10/site-packages (from jinja2->torch->pthflops) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /home/harryk/.local/lib/python3.10/site-packages (from sympy->torch->pthflops) (1.2.1)\n"]}],"source":["!pip install pthflops"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-23T03:12:33.536508Z","iopub.status.idle":"2024-05-23T03:12:33.536813Z","shell.execute_reply":"2024-05-23T03:12:33.536676Z","shell.execute_reply.started":"2024-05-23T03:12:33.536663Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1716013488334,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"ebQJhj-rfYFP","trusted":true},"outputs":[],"source":["# ################################################\n","# import packages\n","\n","import argparse\n","import time\n","import math\n","import random\n","import shutil\n","import sys\n","import glob\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","\n","from pathlib import Path\n","\n","from PIL import Image\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1309,"status":"ok","timestamp":1716013489640,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"uAjGVoVnl1ZK","trusted":true},"outputs":[],"source":["seed = 123                                        # for reproducibility\n","cuda = True                                       # use GPU\n","save = True                                       # save trained model\n","image_dataset = os.path.join(global_path,'train')  # path to the root of the image dataset\n","sequence_dataset = os.path.join(global_path,'Video')  # path to the root of the video dataset\n","checkpoint = ''                                   # load pretrained model\n","epochs = 10                                       # total training epochs\n","clip_max_norm = 1.0                               # avoid gradient explosion\n","patch_size = (256, 256)                           # input size for the training network\n","learning_rate = 1e-4\n","batch_size = 16\n","test_batch_size = 16\n","num_workers = batch_size                          # multi-process for loading training data\n","N = 128\n","M = 192\n","\n","# ################################################\n","# SequenceFolder Dataset\n","\n","class SequenceFolder(Dataset):\n","    \"\"\"Load an image folder database. Training and testing image samples\n","    are respectively stored in separate directories:\n","\n","    .. code-block::\n","\n","        - rootdir/\n","            - train/\n","              - video 1\n","                - img000.png\n","                - img001.png\n","                ...\n","              - video 2\n","                - img000.png\n","                - img001.png\n","                ...\n","            - test/\n","              - video 1\n","                - img000.png\n","                - img001.png\n","                ...\n","              - video 2\n","                - img000.png\n","                - img001.png\n","                ...\n","\n","    Args:\n","        root (string): root directory of the dataset\n","        transform (callable, optional): a function or transform that takes in a\n","            PIL image and returns a transformed version\n","        split (string): split mode ('train' or 'val')\n","    \"\"\"\n","\n","    def __init__(self, root, transform=None, split=\"train\"):\n","        self.mode = split\n","        splitdir = Path(root) / split\n","\n","        if not splitdir.is_dir():\n","            raise RuntimeError(f'Invalid directory \"{root}\"')\n","\n","        self.samples = self.get_all_images(splitdir)\n","\n","        self.transform = transform\n","\n","    def get_all_images(self, direc):\n","        self.images = []\n","        self.image_sequence = []\n","        self.sequences = [f for f in direc.iterdir() if f.is_dir()]\n","        for sd in self.sequences:\n","            images = []\n","            for f in (sd / 'img').iterdir():\n","                if f.is_file():\n","                  images.append(f)\n","            self.image_sequence.append(images)\n","            self.images = self.images + list(sorted(images))\n","\n","        return self.images\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","\n","        Returns:\n","            img: `PIL.Image.Image` or transformed `PIL.Image.Image`.\n","        \"\"\"\n","        img = Image.open(self.samples[index]).convert(\"RGB\")\n","        if self.transform:\n","            return self.transform(img)\n","        return img\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","test_dataset = SequenceFolder(sequence_dataset, split=\"test\", transform=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716013489640,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"63X35flPm5fx","trusted":true},"outputs":[],"source":["\n","\n","\n","\n","class AverageMeter:\n","    \"\"\"Compute running average.\"\"\"\n","\n","    def __init__(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def PSNR(img1, img2):\n","    # img1 and img2 within range [0, 1]\n","    # img1 shape: (B, C, H, W)\n","    # img2 shape: (B, C, H, W)\n","\n","    img1, img2 = img1.detach(), img2.detach()\n","    img1 = img1 * 255\n","    img2 = img2 * 255\n","    batch_size = img1.shape[0]\n","    img1 = img1.reshape(batch_size, -1)\n","    img2 = img2.reshape(batch_size, -1)\n","    mse = torch.mean((img1 - img2) ** 2)\n","    return torch.mean(20 * torch.log10(255.0 / torch.sqrt(mse)))\n","\n","def final_test_epoch(test_dataset, transform, model, criterion):\n","    model.eval()\n","    device = next(model.parameters()).device\n","\n","    mse_loss = AverageMeter()\n","    psnr = AverageMeter()\n","\n","    with torch.no_grad():\n","        for video in test_dataset.image_sequence:\n","            psnr_video = AverageMeter()\n","            for image in video:\n","                image = Image.open(image).convert(\"RGB\")\n","                image = transform(image)\n","                d = image.to(device).unsqueeze(0)\n","                out_net = model(d)\n","                d_out = out_net['x_hat']\n","                out_criterion = criterion(out_net, d)\n","                psnr_video.update(PSNR(d, d_out))\n","\n","            mse_loss.update(out_criterion[\"mse_loss\"])\n","            psnr.update(psnr_video.avg)\n","\n","    print(\n","        f\"Final test of model: Average losses:\"\n","        f\"\\tMSE loss: {mse_loss.avg:.3f}\"\n","        f'\\tSequence-wise PSNR: {psnr.avg: .3f}\\n'\n","    )\n","\n","    return mse_loss.avg\n","\n","class Loss(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, output, target):\n","        out = {}\n","        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n","        out[\"loss\"] = out[\"mse_loss\"] * 255\n","\n","        return out\n","\n","test_transforms = transforms.Compose(\n","    [transforms.CenterCrop(patch_size), transforms.ToTensor()]\n",")\n","\n","criterion = Loss()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1716013489640,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"k63OH5MEuS4V","trusted":true},"outputs":[],"source":["from pthflops import count_ops\n","\n","MAX_GFLOPS = 3000\n","MIN_RATIO = 0.7\n","\n","def benchmark(yournet):\n","    yournet = yournet.cuda()\n","    x = torch.randn((1, 3, 500, 500)).cuda()\n","    try:\n","      yournet.compress(x)\n","    except AttributeError:\n","      print('does your network have a ```def compress(self, input)``` function?')\n","      print('refer to the baseline model in the template')\n","      return\n","\n","    est, _ = count_ops(yournet, x, print_readable=False, verbose=False)\n","    est = int(est / 1e9)\n","    if est <= MAX_GFLOPS:\n","      print('#' * 30)\n","      print('[Acceptable Model Complexity]')\n","      print('#' * 30)\n","      print('\\n')\n","    else:\n","      assert 0, 'Your model complexity is {} GFLOPS, the acceptable maximum is {} GFLOPS. Make your model smaller'.format(est, MAX_GFLOPS)\n","\n","    compressed = yournet.compress(x)\n","    ratio = 1 - compressed.numel() / x.numel()\n","    if ratio >= MIN_RATIO:\n","      print('#' * 30)\n","      print('[Acceptable Compression Ratio]')\n","      print('#' * 30)\n","    else:\n","      assert 0, 'Current compression ratio is {} , the acceptable lowest ratio is {}. Make it higher'.format(ratio, MIN_RATIO)\n","\n","    print('\\n')\n","\n","    print('#' * 30)\n","    print('Final test result of your model')\n","    print('#' * 30)\n","    print('\\n')\n","\n","    loss = final_test_epoch(test_dataset, test_transforms, yournet, criterion)"]},{"cell_type":"markdown","metadata":{"id":"2SZQiXMr9VEo"},"source":["Use the below code if you need to upload the model weight file from your local PC."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"elapsed":31754,"status":"error","timestamp":1716013521391,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"1zEm8wods-Ps","outputId":"9b8297f0-58ad-4813-c558-8c9d8f786552","trusted":true},"outputs":[],"source":["# from google.colab import files\n","\n","# model_weight = files.upload()\n","# filename = next(iter(model_weight))\n","# print(\"Uploaded \" + filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38818,"status":"ok","timestamp":1716013566468,"user":{"displayName":"","userId":""},"user_tz":-600},"id":"XCVcmff2kb3K","outputId":"84f5d81d-da19-47e7-ba0a-686575feee00","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["##############################\n","[Acceptable Model Complexity]\n","##############################\n","\n","\n","##############################\n","[Acceptable Compression Ratio]\n","##############################\n","\n","\n","##############################\n","Final test result of your model\n","##############################\n","\n","\n","Final test of model: Average losses:\tMSE loss: 17.765\tSequence-wise PSNR: -11.255\n","\n"]}],"source":["# an EXAMPLE network to run the test\n","# test_model = Network(N, M)\n","# test_model.load_state_dict(torch.load('checkpoint_best_loss.pth.tar')['state_dict'])\n","# test_model.eval()\n","benchmark(test_model)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuClass":"premium","gpuType":"T4","provenance":[{"file_id":"https://github.com/HarryK4673/ELEC5306A3/blob/main/ELEC_5306_2024_Project_3_Template.ipynb","timestamp":1716013699985}]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5033813,"sourceId":8447689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
